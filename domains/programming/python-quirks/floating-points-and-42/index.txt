## Where The Hell Does 42 Come From?

In your Python 3 console of choice, try entering
```python
>>> int((3 &#42; 0.1 - 0.3) &#42;&#42; -0.1)
```

The answer, surprisingly, comes out as \(42\). No, this isn't an easter egg, and [it's not deliberate](!Debatably. I suppose it obviously isn't *ideal* behaviour, computational cost aside, in the sense that it isn't accurate maths, but it is the best solution all things considered.), though it is predictable behaviour. Let's see exactly why. First, we'll investigate the parts of the computation one by one to see where the error comes in.

```python
>>> 3 &#42; 0.1
0.30000000000000004
```
Okay, I see where this is going already. A classic [floating-point error](https://docs.python.org/3/tutorial/floatingpoint.html)!
```python
>>> 0.30000000000000004 - 0.3
5.551115123125783e-17
>>> from math import log
>>> log(0.30000000000000004 - 0.3, 2)
-54.0
```
Looks like this is equal to \(2^{-54}\). Let's keep that in mind.
```python
>>> (0.30000000000000004 - 0.3) &#42;&#42; -0.1
42.22425314473263
```
This would be \(\left(2^{-54} \right)^{-0.1} = 2^{-54 \cdot -0.1} = 2^{5.4} \approx 42.22\), as expected.
```python
>>> int(42.22425314473263)
42
```
Obviously.

So this is where 42 comes from. But that isn't the whole mystery! Why \(2^{-54}\), and why `0.30000000000000004`? This is a floating point error. Python doesn't store fractions and decimals exactly, it stores them as bits. Let's see the exact bits that are used in this calculation.

Python uses 64-bit [doubles](!Double-precision floating point numbers, which represent a decimal number.) to store non-integer numbers. Using the `struct` module, we can view the byte structure of these floats.

```python
>>> from struct import pack
>>> [i for i in pack("!d", 0.1)]
[63, 185, 153, 153, 153, 153, 153, 154]
>>> [bin(i) for i in pack("!d", 0.1)]
['0b111111', '0b10111001', '0b10011001', '0b10011001', '0b10011001', '0b10011001', '0b10011001', '0b10011010']
```

So `0.1` is stored as
```binary
      63      185      153      153      153      153      153      154
00111111 10111001 10011001 10011001 10011001 10011001 10011001 10011010
││          ││                                                        │
│└ exponent ┘└──────────────────────── mantissa ──────────────────────┘
│
└──── sign bit: 0 for positive, 1 for negative
```

This is according to the [IEEE 754 Standard](https://www.geeksforgeeks.org/ieee-standard-754-floating-point-numbers/).

This stores a number as three parts: a sign, an exponent, and a mantissa. The exponent stores, roughly, how big the number is (as an order of magnitude.) The mantissa stores the precise details. The exponent is 11 bits long, and the mantissa 52.

What's the "real" exact value that's being stored here? Well, let's look at the parts. The exponent is `0b01111111011 = 1019`, but in this specification, `1023` is the "bias", so this is really an exponent of `-4`.

Then we take the mantissa: `0b1001100110011001100110011001100110011001100110011010`. In the specification, this is implicitly prefaced with a `1` to save space, because the leading digit is always a `1`. Thus we can convert

```binary
  add the leading 1 here
  │
0b11001100110011001100110011001100110011001100110011010
                                = 7,205,759,403,792,794
```
and then divide through by 2 a total of 56 times (52 by default, [being the length of the mantissa](!The mantissa is theoretically the *decimal* part of a number. We're treating it as an integer, so have shifted it 52 places to the left: this division undoes that shift.), and an additional 4 for the exponent). This gives us

```division
7205759403792794.00000000000000000000000000000000000000000000000000000000
3602879701896397.00000000000000000000000000000000000000000000000000000000
1801439850948198.50000000000000000000000000000000000000000000000000000000
 900719925474099.25000000000000000000000000000000000000000000000000000000
 450359962737049.62500000000000000000000000000000000000000000000000000000
                                                                     ...
          838860.80000000004656612873077392578125000000000000000000000000
          419430.40000000002328306436538696289062500000000000000000000000
                                                                     ...
             102.40000000000000568434188608080148696899414062500000000000
              51.20000000000000284217094304040074348449707031250000000000
                                                                     ...
               0.40000000000000002220446049250313080847263336181640625000
               0.20000000000000001110223024625156540423631668090820312500
           x = 0.10000000000000000555111512312578270211815834045410156250
```

Whew! That's the *exact* value that we're storing when we put `0.1` into Python. How far off is it? Well, notice that \(x = 0.1 + 0.1 \cdot 2^{-54}\), which gives us our error term.

The number `3` is stored precisely in Python, being an integer. Multiplying by 3 is therefore a [shift 1 place to the left](!which multiplies by 2) and then a bitwise addition with the original number. I won't bore you with the details, but we get

```binary
      63      211       51       51       51       51       51       52
00111111 11010011 00110011 00110011 00110011 00110011 00110011 00110100
-----------------------------------------------------------------------
            = 0.3000000000000000444089209850062616169452667236328125000
```

Okay, that explains the `0.30000000000000004`. What about when we put in `0.3` directly?

```binary
      63      211       51       51       51       51       51       51
00111111 11010011 00110011 00110011 00110011 00110011 00110011 00110011
-----------------------------------------------------------------------
            = 0.2999999999999999888977697537484345957636833190917968750
```

A-ha! [The very last bit is different!](!Thereby changing the preceding two, but that's incidental.) This gives us a very slightly different value: when Python wants to store `0.3`, it actually stores `0.2999999...750`. The difference between these two values is exactly the \(2^{-54}\) we saw earlier, which has now been carried through.

So when the subtraction occurs, and we calculate `3 * 0.1 - 0.3`,
what we're *really* doing is calculating

```subtraction
   0.3000000000000000444089209850062616169452667236328125000
-  0.2999999999999999888977697537484345957636833190917968750
------------------------------------------------------------
=  0.0000000000000000555111512312578270211815834045410156250
```

which is precisely \(2^{-54}\). Then, when we raise this to the power of \(-0.1\), we are really computing \(2^{-54 \cdot -0.1} = 2^{5.4} \approx 42.22\). Truncating this gives us \(42\), as we observed.

So that's where the mysterious \(42\) comes from - an errant bit which is very slightly different in two representations of what should be `0.3`!