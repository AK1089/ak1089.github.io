<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AK1089's Website</title>
    <link rel="icon" type="image/x-icon" href="../../../images/favicon.ico">
    <link rel="stylesheet" href="../../../styles/styles.css">
    <link rel="stylesheet" href= "styles.css">
    <script src="../../../scripts/script.js"></script>
    <script src= "script.js"></script>
    <link rel="stylesheet" href= "https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css">
    <script src= "https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.js" defer></script>
    <script src= "https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/contrib/auto-render.min.js" defer></script>
</head>
<body onload="on_body_load()">
    <div id="content-container">
        <a href="../../../" id="home-icon">
            <img src="../../../images/home.png" alt="Home" width="40" height="40" class="icon home-icon">
        </a>
        <a href="../../..//map" id="map-icon">
            <img src="../../../images/map.png" alt="Home" width="40" height="40" class="icon map-icon">
        </a>
        <div id="main-content">
            <p><h2 id="memorising-the-gaussian-distribution">Memorising the Gaussian Distribution</p></h2>
<p>The <em>Gaussian</em>, or "normal", distribution crops up a lot in statistics and probability. The single-variable form of the Gaussian is given by
\[ f_X\left(x\right)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^{2}}\left(x-\mu\right)^{2}\right)\]
where \(\mu\) is the mean of the distribution, and \(\sigma^2\) is the variance.</p>
<p>Since so many things in life are normally distributed, it's important to have a handle on this distribution. Here, we're going to figure out how to quickly estimate probabilities and percentiles of a normally distributed variable.</p>
<p><h3 id="the-standard-normal-distribution">The Standard Normal Distribution</p></h3>
<p>The <em>standard normal</em> distribution has \(\mu = 0\) and \(\sigma^2 = 1\). It looks like this:
<div class="image-container"><img src="standard-gaussian.png" alt="Figure 1: The standard normal distribution. Note the symmetry about the y-axis."><div class="caption">Figure 1: The standard normal distribution. Note the symmetry about the y-axis.</div></div></p>
<p>The important thing to note is that <em>every</em> normal distribution looks like this, just shifted and scaled. Given a normally distributed variable \(X\) with mean \(\mu\) and standard deviation \(\sigma\), we have:
\[\mathbb{P}\left[a\le Z\le b\right]=\mathbb{P}\left[\frac{a-\mu}{\sigma}\le X\le\frac{b-\mu}{\sigma}\right]\]
where \(Z \sim N(0, 1)\) (the standard normal).</p>
<p>This means we only need to memorise the quantiles of the standard normal, and can easily convert our answers when we need to.</p>
<p><h3 id="notation">Notation</p></h3>
<p>We use \(F(x)\) to denote the cumulative probability function evaluated at \(x\), ie. \(\mathbb{P}\left[X\le x\right]\), where \(X \sim N(0, 1)\).</p>
<p>We use \(\Phi(x)\) to denote the inverse function: if \(F(x) = c\), then \(\Phi(c) = x\). (Looking at the definition of the normal distribution, and noticing that \(f_X\) is a positive function, we see that \(F\) is indeed a bijection \(\mathbb{R} \to (0, 1)\), and so this inverse really does exist.) By convention, \(\Phi(0) = -\infty\) and \(\Phi(1) = \infty\).</p>
<p><h3 id="properties-of-the-standard-normal">Properties of the Standard Normal</p></h3>
<p>I'd rather not memorise hundreds of numbers, and I'm sure you feel the same way. So, let's find a way to cut down on what we have to learn. What are some properties we can leverage to our advantage here?
<ol>
<li><strong>Symmetry</strong>: the normal distribution is symmetric.</li>
<ul>
<li>This means we can learn only the positive side of the distribution, and the negative side will fall out automatically.</li>
<li>Specifically, \(\Phi(x) = p\), so \(\Phi(-x) = 1-p\).</li>
<li>This implies that \(F(0) = 0.5\).</li>
</ul>
<li><strong>Monotonicity</strong>: both \(F\) and \(\Phi\) are monotonically increasing functions.</li>
<ul>
<li>\(\mathbb{P}\left[a\le X\le b\right] = \Phi(b) - \Phi(a)\), so learning \(\Phi\) lets us calculate all relevant probabilities.</li>
<li>\(\Phi(a) \le \Phi(b) \iff a \le b \iff F(a) \le F(b)\), so we can bound our answers.</li>
</ul>
<li><strong>Rapid drop-off</strong>: almost all of the probability mass of the standard normal is within 4 standard deviations of the mean.</li>
<ul>
<li>\(\Phi(4) = 0.999968\), so \(\Phi(-4) = 0.000032\), so \(\mathbb{P}\left[-4\le X\le 4\right] = 0.999936\) (over 99.99% chance).</li>
</ul>
</ol></p>
<p><h3 id="memorising">Memorising</p></h3>
<p>With all this information in mind, let's memorise some key rules.</p>
<p>We're going to be learning an approximation for \(100(\Phi(x) - 0.5)\) for positive \(x\). This is the <em>percentage</em> probability of our random variable being between \(0\) (the mean) and our value: this is useful because we can either double it (for the probability our random variable is in the range \(\pm x\)) or add \(0.5\) (for the probability our random variable is outright less than \(x\)).</p>
<p>\[
100(\Phi(x) - 0.5) \approx \begin{cases} 40x & 0 \le x \le 0.5 \\ 30x + 4 & 0.5 \le x \le 1 \\ 20x + 14 & 1 \le x \le 1.5 \\ 10x + 28 & 1.5 \le x \le 2 \\ 3x + 42 & 2 \le x \le 2.5 \end{cases}
\]</p>
<p>How accurate is this? Let's take a look on a graph, plotting straight red approximation lines against the true CDF in black.
<div class="image-container"><img src="accuracy.png" alt="Figure 2: The accuracy of this approximation. Pretty good!"><div class="caption">Figure 2: The accuracy of this approximation. Pretty good!</div></div></p>
<p>In fact, you're <em>never</em> more than 1% out.</p>
<p>As a tip to memorise this: the ranges are constant with a width of one half. The coefficients of \(x\) tick down neatly (except the \(3\) at the end), and the last three numbers follow the arithmetic progression \(14, 28, 42\).</p>
<p>These are, of course, not the <em>most accurate</em> numbers, but I think they strike the best balance between accuracy and ease of committing to memory.</p>
<p><h3 id="an-example">An Example</p></h3>
<p><em>The height of men in the UK is (roughly) normally distributed with a height of 175.3cm and a standard deviation of 7.1 cm. How many men are taller than 6'1"?</em></p>
<p>Let's approximate this. 6'1" is about 185cm, which is about 10cm above mean, which is about 1.4 standard deviations. Use the \(20x + 14\) part, getting us about 28 + 14 = 42. So adding 50, we get 92% of men below 6'1", ie 8% above. This took me about 15 seconds in my head, all-in-all.</p>
<p>Putting these parameters into an actual calculator, we get a probability of 7.7%. Not bad at all!</p>
<p></p>

        </div>
        <h6 id="copyright">&#169; Avish Kumar 2024</h6>
    </div>
</body>
</html>
