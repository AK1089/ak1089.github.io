## A Guide To Continuous Probability Distributions

A continuous probability distribution is a function \(f: \mathbb{R} \to \mathbb{R}_{\geqslant 0} \) which describes a random variable \(X\). When a sample of \(X\) is taken (ie. "actually randomising"), the probability is governed by the distribution, according to the law
\[ \mathbb{P}[a \leqslant X \leqslant b] = \int_a^b f(x) \, dx \quad (\text{for } a \leqslant b).\]
Sometimes, this distribution is instead equivalently given in terms of the "cumulative distribution function" \(F: \mathbb{R} \to [0, \, 1]\), defined so that
\[ F(t) = \mathbb{P}[X \leqslant t] = \int_{-\infty}^t f(x) \, dx \quad \text{and} \quad F'(t) = f(t) \text{ with } \lim_{x \to -\infty} F(x) = 0, \, \lim_{x \to \infty} F(x) = 1. \]
There are, of course, infinitely many such distributions. Here, we'll focus on the famous ones, usually given special names.

"""info #B42619 #EEEEEE
Jump to the section on any distribution here:
<ul>
- [Uniform distribution](#the-uniform-distribution)
- [Exponential distribution](#the-exponential-distribution)
- [Normal distribution](#the-normal-distribution)
- [Log-normal distribution](#the-log-normal-distribution) (coming soon)
- [Chi-squared distribution](#the-chi-squared-distribution) (coming soon)
- [t-distribution](#the-t-distribution) (coming soon)
- [F-distribution](#the-f-distribution) (coming soon)
- [Beta distribution](#the-beta-distribution)
- [Gamma distribution](#the-gamma-distribution) (coming soon)
- [Cauchy distribution](#the-cauchy-distribution) (coming soon)
- [Weibull distribution](#the-weibull-distribution) (coming soon)
- [Frechet distribution](#the-frechet-distribution) (coming soon)
- [Pareto distribution](#the-pareto-distribution) (coming soon)
- [Logistic distribution](#the-logistic-distribution) (coming soon)
- [Laplace distribution](#the-laplace-distribution) (coming soon)
- [Gumbel distribution](#the-gumbel-distribution) (coming soon)
- [Hyperbolic distribution](#the-hyperbolic-distribution) (coming soon)
</ul>

For an explanation of the terms used here, see here:
<ul>
- also coming soon!
</ul>
"""

<br>
<hr>
<br>

#### The Uniform Distribution

The uniform distribution is extremely simple: heuristically, every same-sized subinterval within a finite support interval has an equal probability of occurring. It was first named by James Victor Uspensky in his 1937 book *Introduction to Mathematical Probability*, though it has been used for centuries beforehand.
\[
    \begin{align*}
    \text{PDF}: f(x) &=
        \begin{cases}
        \frac{1}{b-a} & a \leqslant x \leqslant b \\
        0 & \text{otherwise}
        \end{cases} \\
    \text{CDF}: F(x) &=
        \begin{cases}
        0 & x \leqslant a \\
        \frac{x-a}{b-a} & a \leqslant x \leqslant b \\
        1 & b \leqslant x
        \end{cases} \\
    \end{align*}
 \]

The distribution is parameterised by two real numbers:
<ul>
- \(a \in \mathbb{R}\) is the *start*. It controls the lower bound of the distribution.
- \(b > a\) is the *end*. It controls the upper bound of the distribution.
</ul>
A standard uniform distribution (usually implemented as the basic "random" function on computers) takes \(a = 0\) and \(b = 1\).

The moments of the uniform distribution are:
\[
    \begin{align*}
    \text{Mean} &= \frac{1}{2}(a+b) \\
    \text{Variance} &= \frac{1}{12}(b-a)^2 \\
    \text{Skewness} &= 0 \\
    \text{Kurtosis} &= 1.8 \\
    \mathbb{E}\left[ \left( \frac{X-\mu}{\sigma} \right)^n \, \right] &=
        \begin{cases}
            \frac{\sqrt{3}^{n}}{n+1} & n \text{ even} \\
            0 & n \text{ odd} \\
        \end{cases} \\
    \end{align*}
\]
with the quantiles given by
\[
    \begin{align*}
        Q(p) = F^{-1}(p) &= a + p(b-a) \\
        \text{Median} &= Q(1/2) = \frac{1}{2}(a + b) \\
        \text{Mode} &= \text{any value in } [a, \, b] \\
        \text{Support} &= [a, \, b]
    \end{align*}
 \]

The uniform distribution has a few basic properties:
<ul>
- Uniformity: the PDF is constant across the support, so any subinterval \(I = [x, \, x+l] \subseteq [a, \, b]\) of length \(l\) has probability \(\mathbb{P}[X \in I] = \frac{l}{b-a}\).
- **Symmetry**: as a result, the PDF is symmetrical about the mean of the distribution.
- **Coincident Averages**: the mean and median of the distribution coincide, and this value is also modal.
- **Linearity**: the CDF of the uniform distribution is linear on the entire support. In fact, this is the *only* distribution with this property.
- **Invariances**: if \(X\) is uniformly distributed, so is \(\lambda X + c\) for any \(\lambda \neq 0\).
</ul>

Here are some related distributions:
"""info #092050 #EEEEEE
Suppose \(X_1, X_2, X_3, \ldots \) is an iid. sequence of standard uniform random variables. Then
<ul>
- The exponentiation \(X_k^n\) follows a [Beta distribution](#the-beta-distribution) with shape parameters \(\alpha = 1/n\) and \(\beta = 1\).
- In fact, the uniformly distributed \(X_k\) is a special case of the Beta distribution with shape and scale \(1\).
- The average \(\frac{1}{n} \sum_{k=1}^n X_k\) follows a standard [Bates distribution](#the-bates-distribution).
- The sum \(X_1 + X_2\) follows a [triangular distribution](#the-triangular-distribution) with start \(0\), end \(2\), and mode \(1\).
- The absolute difference \(|X_1 - X_2|\) follows another triangular distribution with start \(0\), end \(1\), and mode \(0\).
- The scaling \(l X_1 + a\) for \(l > 0\) is itself uniformly distributed with start \(a\) and end \(a + l\).
</ul>
"""

The uniform distribution frequently arises in situations where there's no bias towards any particular value. For example, in a well-mixed thermal bath, the starting phase of oscillating particles is uniformly distributed in the interval \([0, \, 2\pi]\).

<br>
[↑ back to top](#a-guide-to-continuous-probability-distributions)
<hr>
<br>

#### The Exponential Distribution

The exponential distribution models the time between events in a Poisson point process. It is characterized by a constant failure rate, meaning the probability of an event occurring in the next instant is independent of how much time has already passed. The distribution was first introduced by Agner Krarup Erlang in 1909 in the context of telephone call analysis.
\[
    \begin{align*}
    \text{PDF}: f(x) &=
        \begin{cases}
        \lambda e^{-\lambda x} & x \geqslant 0 \\
        0 & x < 0
        \end{cases} \\
    \text{CDF}: F(x) &=
        \begin{cases}
        1 - e^{-\lambda x} & x \geqslant 0 \\
        0 & x < 0
        \end{cases} \\
    \end{align*}
 \]

The distribution is parameterised by one real number:
<ul>
- \(\lambda > 0\) is the *rate*. It controls the scale of the distribution.
</ul>
A standard exponential distribution takes \(\lambda = 1\).

The moments of the exponential distribution are:
\[
    \begin{align*}
    \text{Mean} &= \frac{1}{\lambda} \\
    \text{Variance} &= \frac{1}{\lambda^2} \\
    \text{Skewness} &= 2 \\
    \text{Kurtosis} &= 9 \\
    \mathbb{E}\left[ X^n \right] &= n! / \lambda^{n} \\
    \mathbb{E}\left[ \left( \frac{X-\mu}{\sigma} \right)^n \, \right] &= \sum_{k=0}^{n}\left(\frac{n!}{\left(n-k\right)!}\cdot\left(-1\right)^{n-k}\right) \\
    &=\; !n \text{ (the number of \textit{derangements} of } n \text{ elements)} \\
    &= \left\lfloor \frac{n!}{e}+\frac{1}{2} \right\rfloor
    \end{align*}
\]
with the quantiles given by
\[
    \begin{align*}
        Q(p) = F^{-1}(p) &= -\frac{1}{\lambda}\ln(1-p) \\
        \text{Median} &= Q(1/2) = \frac{\ln(2)}{\lambda} \\
        \text{Mode} &= 0 \\
        \text{Support} &= [0, \, \infty)
    \end{align*}
 \]

The exponential distribution has a few basic properties:
<ul>
- **Memorylessness**: the probability and expectation from a given time is independent of how much time has already passed. Specifically,
<ul>
- \(\mathbb{P}[X \leqslant t + s \mid X \geqslant t] = \mathbb{P}[X \leqslant s]\) for every \(t, s \geqslant 0\).
- \(\mathbb{E}[X \mid X \geqslant t] = t + \frac{1}{\lambda}\) for every \(t \geqslant 0\).
- This property is unique to the exponential distribution. 
</ul>
- **Constant Hazard Rate**: the *hazard rate* \(h(x) = \frac{f(x)}{1-F(x)} = \lambda\) is constant.
- **Maximum Entropy**: among all continuous distributions with support \(&#91;0, \, \infty)\) and mean \(\mu\), the \(\mathrm{Exponential}\left(\frac{1}{\mu}\right)\) distribution has the highest entropy.
- **Invariances**: if \(X\) is exponentially distributed, so is \(cX\) for any \(c > 0\).
</ul>

Here are some related distributions:
"""info #092050 #EEEEEE
Suppose \(X_1, X_2, X_3, \ldots \) is an iid. sequence of exponential random variables with rate \(\lambda\). Then
<ul>
- The sum \(\sum_{k=1}^n X_k\) follows a [Gamma distribution](#the-gamma-distribution) with shape \(n\) and rate \(\lambda\).
- In fact, the exponentially distributed \(X_k\) is a special case of the Gamma distribution with shape \(1\) and rate \(1\)
- The minimum \(\min(X_1, \ldots, X_n)\) follows an exponential distribution with rate \(n\lambda\).
- The ratio \(X_1 / (X_1 + X_2)\) follows a [Beta distribution](#the-beta-distribution) with shape parameters \(\alpha = \beta = 1\).
- For a Poisson process with rate \(\lambda\), the times between events are distributed exponentially with rate \(\lambda\).
- The scaling \(c X_1\) for \(c > 0\) is itself exponentially distributed with rate \(\lambda/c\).
</ul>
"""

The exponential distribution frequently arises in situations involving the time between events, particularly when events occur continuously and independently at a constant average rate. For example, it's used to model the time between radioactive decay events.

<br>
[↑ back to top](#a-guide-to-continuous-probability-distributions)
<hr>
<br>

#### The Normal Distribution

The normal distribution, also known as the Gaussian distribution, is a fundamental continuous probability distribution. It is symmetric about its mean and follows a characteristic "bell curve" shape. The distribution was first introduced by Abraham de Moivre in 1733, but gained prominence through the work of Carl Friedrich Gauss in the early 19th century, after which it is often named.
\[
    \begin{align*}
    \text{PDF}: f(x) &= \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2\right) \\
    \text{CDF}: F(x) &= \Phi(x) = \frac{1}{2}\left[1 + \mathrm{erf}\left(\frac{x-\mu}{\sigma\sqrt{2}}\right)\right] \\
    \end{align*}
 \]
As the PDF does not have a closed-form antiderivative, the CDF is usually given in terms of the [error function](https://en.wikipedia.org/wiki/Error_function) \(\mathrm{erf}\). It is given the standard name \(\Phi(x)\).

The distribution is parameterised by two real numbers:
<ul>
- \(\mu \in \mathbb{R}\) is the *mean*. It controls the location of the centre of the distribution (which is its mean, median and mode).
- \(\sigma^2 > 0\) is the *variance* (where \(\sigma\) is the *standard deviation*). It controls the scale of the distribution.
</ul>
A standard normal distribution takes \(\mu = 0\) and \(\sigma = 1\).

The moments of the normal distribution are:
\[
    \begin{align*}
    \text{Mean} &= \mu \\
    \text{Variance} &= \sigma^2 \\
    \text{Skewness} &= 0 \\
    \text{Kurtosis} &= 3 \\
    \mathbb{E}\left[ \left( \frac{X-\mu}{\sigma} \right)^n \, \right] &=
        \begin{cases}
            \sigma^n (n-1)!! & n \text{ even} \\
            0 & n \text{ odd} \\
        \end{cases} \\
    \end{align*}
\]
(note the \(!!\) denoting the [double factorial](https://en.wikipedia.org/wiki/Double_factorial)), with the quantiles given by
\[
    \begin{align*}
        Q(p) = F^{-1}(p) &= \mu + \sigma\sqrt{2} \; \mathrm{erf}^{-1}(2p-1) \\
        \text{Median} &= \mu \\
        \text{Mode} &= \mu \\
        \text{Support} &= (-\infty, \, \infty)
    \end{align*}
 \]

The normal distribution has a few basic properties:
<ul>
- **Symmetry**: the PDF is symmetrical about the mean of the distribution.
- **Coincident Averages**: the mean, median, and mode of the distribution are all unique and all coincide.
- **Maximum Entropy**: among all real-valued distributions with specified mean and variance, the normal distribution has the largest entropy.
- **Invariances**: if \(X\) is normally distributed, so is \(aX + b\) for any \(a \neq 0\) and \(b\).
- **Limiting Distribution**: by the Central Limit Theorem, for *any* sequence of iid. non-constant random variables with finite mean \(\mu\) and variance \(\sigma^2\),
\[ \lim_{n \to \infty} \mathbb{P} \left( \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} \leq z \right) = \Phi(z) \]
</ul>

Here are some related distributions:
"""info #092050 #EEEEEE
Suppose \(X_1, X_2, X_3, \ldots \) is an iid. sequence of standard normal random variables. Then
<ul>
- The sum \(\sum_{k=1}^n X_k\) follows a normal distribution with mean \(0\) and variance \(n\).
- The sum of squares \(\sum_{k=1}^n X_k^2\) follows a [Chi-squared distribution](#the-chi-squared-distribution) with \(n\) degrees of freedom.
- The ratio \(X_1 / X_2\) follows a [Cauchy distribution](#the-cauchy-distribution).
- The scaling \(a X_1 + b\)is itself normally distributed with mean \(b\) and variance \(a^2\).
</ul>
"""

The normal distribution frequently arises in natural and social sciences due to the central limit theorem, which states that the sum of many independent random variables tends towards a normal distribution. It's used to model various phenomena, from heights and weights in a population to measurement errors in scientific experiments.

<br>
[↑ back to top](#a-guide-to-continuous-probability-distributions)
<hr>
<br>

#### The Beta Distribution

The beta distribution is a family of continuous probability distributions defined on the interval \([0, 1]\). It is extremely versatile and can take on a variety of shapes depending on its parameters. The distribution was first studied by Euler and Bernoulli in the 18th century and has since found applications in Bayesian statistics.

Firstly, here are two useful functions used in the definition of the Beta distribution.
\[
    \begin{align*}
    \Gamma(z) &= \int_0^\infty t^{z-1} e^{-t} \, dt = (n-1)! \text{ for positive integers } n \\
    B\left(x_{1},x_{2}\right)&=\frac{\Gamma\left(x_{1}\right)\Gamma\left(x_{2}\right)}{\Gamma\left(x_{1}+x_{2}\right)} \\
    \end{align*}
\]
\[
    \begin{align*}
    \text{PDF}: f(x) &=
        \begin{cases}
        \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)} & 0 \leqslant x \leqslant 1 \\
        0 & \text{otherwise}
        \end{cases} \\
    \text{CDF}: F(x) &= I_x(\alpha,\beta) \text{ (no closed form)} \\
    \end{align*}
 \]

The distribution is parameterised by two positive real numbers:
<ul>
- \(\alpha > 0\) is the first *shape parameter*.
- \(\beta > 0\) is the second *shape parameter*.
</ul>

The moments of the beta distribution are:
\[
    \begin{align*}
    \text{Mean} &= \frac{\alpha}{\alpha+\beta} \\
    \text{Variance} &= \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \\
    \text{Skewness} &= \frac{2(\beta-\alpha)\sqrt{\alpha+\beta+1}}{(\alpha+\beta+2)\sqrt{\alpha\beta}} \\
    \text{Kurtosis} &= 3 + \frac{6((\alpha - \beta)^2 (\alpha + \beta + 1) - \alpha \beta (\alpha + \beta + 2))}{\alpha \beta (\alpha + \beta + 2) (\alpha + \beta + 3)} \\
    \mathbb{E}\left[ X^n \right] &= \frac{B(\alpha+n,\beta)}{B(\alpha,\beta)} = \frac{\Gamma\left(\alpha+n\right)}{\Gamma\left(\alpha+n+\beta\right)}\cdot\frac{\Gamma\left(\alpha+\beta\right)}{\Gamma\left(\alpha\right)} \\
    \end{align*}
\]
with the quantiles given by
\[
    \begin{align*}
        Q(p) = F^{-1}(p) &= I^{-1}_p(\alpha,\beta) \\
        \text{Median} &\approx \frac{3\alpha-1}{3\alpha+3\beta-2} \\
        \text{Mode} &=
        \begin{cases}
        \text{every value} & \text{for } \alpha = \beta = 1 \\
        \frac{\alpha-1}{\alpha+\beta-2} & \text{for } \alpha,\beta \geqslant 1 \\
        0 \text{ and } 1 & \text{for } \alpha,\beta < 1 \\
        1 & \text{for } \alpha < 1 \\
        0 & \text{for } \beta < 1 \\
        \end{cases} \\
        \text{Support} &= [0, \, 1]
    \end{align*}
 \]

The beta distribution has a few basic properties:
<ul>
- **Flexibility**: It can take on a wide variety of shapes depending on its parameters, including U-shaped, uniform, and bell-shaped.
- **Symmetry**: When \(\alpha = \beta\), the distribution is symmetric about 0.5.
- **Invariances**: If \(X\) follows a beta distribution with parameters \(\alpha\) and \(\beta\), then \(1-X\) follows a beta distribution with parameters \(\beta\) and \(\alpha\).
- **Order Statistics**: The beta distribution arises naturally as the distribution of order statistics from the uniform distribution.
</ul>

Here are some related distributions:
"""info #092050 #EEEEEE
Suppose \(X\) follows a beta distribution with parameters \(\alpha\) and \(\beta\). Then
<ul>
- If \(\alpha = \beta = 1\), \(X\) follows a [uniform distribution](#the-uniform-distribution) on \([0, 1]\).
- If \(Y_1, Y_2\) follow a [gamma distribution](#the-gamma-distribution) with shapes \(\alpha, \beta\) and rate \(1\), then \(\frac{Y_1}{Y_1+Y_2}\) follows a beta distribution with parameters \(\alpha\) and \(\beta\).
- If \(Y\) follows an [F-distribution](#the-f-distribution) with parameters \(d_1\) and \(d_2\), then \(X = \frac{d_1Y}{d_1Y+d_2}\) follows a beta distribution with parameters \(\frac{d_1}{2}\) and \(\frac{d_2}{2}\).
- If \(Y_1, Y_2, Y_3, \ldots Y_n\) is a sequence of iid. standard uniform random variables, then the \(k^{th}\) smallest value follows a beta distribution with parameters \(k\) and \(n-k+1\).
</ul>
"""

The beta distribution frequently arises in Bayesian inference as a [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior) for binomial and Bernoulli distributions. For more on this, see my article on [Laplace's Rule of Succession](../laplaces-rule-of-succession). It's used to model the distribution of probabilities, proportions, and random variables limited to intervals of finite length. For example, it can model the distribution of the proportion of time a machine is in use during a day, or the distribution of allele frequencies in population genetics. 

<br>
[↑ back to top](#a-guide-to-continuous-probability-distributions)
<hr>
<br>