## Laplace's Rule of Succession

(jump to the [interactive model](#interactive-model))

You're handed a (possibly biased) coin, and you flip it three times. It comes up tails, then heads, then heads again. What's (your best guess for) the probability it comes up heads on the fourth toss? Most people instinctively say two in three, but according to Laplace's Rule of Succession, it's actually three in five, or 60%. What?

Okay, let's take a step back. Firstly, I want you to shake the intuition that says it should be two in three. There's actually a pretty easy intuition pump which disproves this idea. Suppose instead that the first toss was also heads. Then, would you really say that the coin's chance of landing heads a fourth time was three in three? That you were 100% certain, more certain than you've ever been in your life, that it just *had* to come up heads again?

No. No you wouldn't.

So two in three is wrong too. What's right?

### Priors and Posteriors

We're assuming here that the coin has some fixed (but unknown) real probability \(p\) of landing heads (and thus lands tails with probability \(1-p\)). This does preclude "trick coins", which land alternately on heads and tails, or land on heads on prime-numbered tosses, or whatever, but those aren't real coins anyway.

What's your distribution over \(p\)? Well, before you toss the coin, you have absolutely no clue. Let's say it can be any real number [between](!I'm including the endpoints here, not that it matters. If you don't like the idea of certainty, you can pretend I excluded them: the maths works out the same way.) zero and one, and your credence is uniformly distributed. This is called the uniform prior.

Ah, you might say. Now we can use Bayes' Rule to update our distribution, and then pick \(p\) out from there! And yes, we can do that. In fact, let's do it!

The likelihood of heads under our assumption \(p = p_0\) is \(p_0\), and the likelihood of tails is \(1-p_0\). So, if we toss \(n\) coins and get heads \(r\) times, the likelihood is
\[L(p_0) = \mathbb{P}(r \text{ heads}, \; n-r \text{ tails} \mid p = p_0) = p_0^r \cdot (1-p_0)^{n-r} \cdot \binom{n}{r}\]
(technically our observation is ordered, so doesn't have the last binomial factor, but this scales *every* value by the same constant factor, so we can ignore it.)

Since our prior was uniform, the posterior distribution of our beliefs over \(p\) will be directly proportional to the likelihood, just normalised. What's this normalisation constant? It will be the reciprocal of the integral of \(L(p)\) from zero to one.
\[
    \begin{align*}
    \int_0^1 L(p) \, \text{d}p &= \int_0^1 p_0^r \cdot (1-p_0)^{n-r} \cdot \binom{n}{r} \, \text{d}p \\
    &= \frac{n!}{r! \cdot (n-r)!} \; \int_0^1 p_0^r \cdot (1-p_0)^{n-r}  \, \text{d}p \\
    &= \frac{n!}{r! \cdot (n-r)!} \cdot \frac{r!\cdot(n-r)!}{\left(n+1\right)!} = \frac{1}{n+1}
    \end{align*}
\]

Okay, so we need to multiply everything by \(n+1\). This means our expression is
\[f(p) = (n+1) \cdot p^r \cdot (1-p)^{n-r} \cdot \binom{n}{r}\]
where \(f\) is our credence function. Looking more closely at this function, we notice it takes the former
\[
    \begin{align*}
    f(p) &= p^r \cdot (1-p)^{n-r} \cdot \frac{n! \cdot (n+1)}{r! \cdot (n-r)!} \\
    f(p) &= p^r \cdot (1-p)^{n-r} \cdot \frac{\Gamma\left(n+2\right)}{\Gamma\left(r+1\right)\Gamma\left(n-r+1\right)} \\
    f(p) &= p^{(r+1)-1} \cdot (1-p)^{(n-r+1)-1} \cdot B\left(r+1,\ n-r+1\right)
    \end{align*}    
\]
which is precisely the form of the [Beta distribution](../all-probability-distributions#the-beta-distribution) with parameters \(r+1\) and \(n-r+1\).

### Estimating and Predicting

And now, you say, we find the maximum likelihood estimator, ie. the maximum of this function \(f\). This occurs when the derivative \(\frac{df}{dp} = 0\). Luckily, for this constraint we can take out constant factors, and find the stationary points of \(p^{r} \cdot (1-p)^{n-r} \) instead. Let's do that.
\[
    \begin{align*} \frac{df}{dp} &= \frac{d}{dp} \left( p^{r}\cdot\left(1-p\right)^{n-r} \right) \\
    &= rp^{r-1}\cdot\left(1-p\right)^{n-r}-p^{r}\cdot\left(n-r\right)\left(1-p\right)^{n-r-1} \\
    &= \left(p^{r-1}\cdot\left(1-p\right)^{n-r-1}\right)\left(r\left(1-p\right)-p\left(n-r\right)\right) = 0
    \end{align*}
\]
The first term is zero only when \(p\) is at an extreme, ie. \(0\) or \(1\). However, at these points \(f(p)\) itself is zero too. However, \(f\) is [not identically zero](!We know this as it integrates to one across its domain, and also because it's obvious.), and so these cannot be the maximisers of \(f\).

Thus we require the second term to be zero. This occurs when
\[
    \left(r\left(1-p\right)-p\left(n-r\right)\right) = r-pr-pn+pr = r - pn = 0  \implies p = \frac{r}{n}
\]
Whew! But wait a second, this is what we had at the beginning, and we said it was wrong! Where has the mistake come from?

It was actually in the very last step. Our Bayesian update was right, our credence function is right, and our maximum likelihood estimator for \(p\) is right. But we're not looking to guess the value of \(p\), we're looking to guess the *probability the coin comes up heads next*.

Yes, of course this is equal to \(p\), but there's a subtle difference. When guessing the former, we want the modal value of \(p\) (to maximise our chance of getting our guess right). And when we're guessing the probability of the coin coming up heads, this is a meaningless quantity! What we instead want is the *expected value* of \(p\).

This is, of course, found by integrating \(\int_0^1 p \cdot f(p) \, \text{d}p\). I'll spare you the tedious calculation, but it comes out as
\[ \mathbb{P}[\text{heads}] = \mathbb{E}[p] = \int_0^1 p \cdot f(p) \, \text{d}p = \int_0^1 p \cdot (n+1) \cdot p^r \cdot (1-p)^{n-r} \cdot \binom{n}{r} \, \text{d}p = \frac{\left(n+1\right)!}{r!\cdot\left(n-r\right)!}\int_{0}^{1}p^{r+1}\cdot\left(1-p\right)^{n-r} \, \text{d}p = \frac{r+1}{n+2}\]

Okay! Well, that's a neat result. In fact, this is precisely Laplace's rule of succession.

[&An excerpt from *Essai philosophique sur les probabilités*, written by Laplace in 1840.](original-essay.png)

He writes

"""info #092050 #EEEEEE
On trouve ainsi qu'un évènement étant arrivé de suite un nombre quelconque de fois, la probabilité qu'il arrivera encore la fois suivante est égale à ce nombre augmenté de l'unité, divisé par le même nombre augmenté de deux unités.
"""

which translates to
"""info #B42619 #EEEEEE
We thus find that when an event has occurred in succession any number of times, the probability that it will occur again the next time is equal to this number increased by one, divided by the same number increased by two units.
"""

### Interactive Model

Here's an interactive version to play around with! Use the sliders to control the values of \(n\) and \(r\).

As you can see, our credence function \(f\) peaks at \(p = r/n\). But the skew of the curve means the expectation is closer to the middle, which means our guess is slightly different.

<iframe src="https://www.desmos.com/calculator/b4lbmtm4ks?embed" width="100%" height="768" style="border: 1px solid #ccc; border-radius: 10px" frameborder=0></iframe>